{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB # best\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1700 entries, 0 to 1699\n",
      "Data columns (total 11 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   ben     1700 non-null   object\n",
      " 1   guj     1700 non-null   object\n",
      " 2   hin     1700 non-null   object\n",
      " 3   kan     1700 non-null   object\n",
      " 4   mal     1700 non-null   object\n",
      " 5   ori     1700 non-null   object\n",
      " 6   pan     1700 non-null   object\n",
      " 7   tam     1700 non-null   object\n",
      " 8   tel     1700 non-null   object\n",
      " 9   urd     1700 non-null   object\n",
      " 10  eng     1700 non-null   object\n",
      "dtypes: object(11)\n",
      "memory usage: 146.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df_test1 = pd.read_csv(r\"D:\\koding\\codes\\Machine Learning\\ICTC\\data\\english-translation (2).csv\")\n",
    "df = pd.read_csv(r\"D:\\koding\\codes\\Machine Learning\\ICTC\\data\\train.csv\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>seta to khubi bhaal have!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>m mase kono ullekhayogya tapapravaher dasha an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ei sabkatai darun lagno shunate.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tar prabandh, ya ki na tar agami bayer ekati u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bartaman mammla, njity taar ekhtiar parityag k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>1997lo sanyo \"pioessiepi\" palimar tantelu chip...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>nenu i pradeshal gurinchi ippatike chala chadi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>gurtunda, edadi kindat me kolig biknu kalcharu.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>bharatvaesha gopp vaividhya-vyatyasabharit bhumi.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>repe ambedkar jayanti!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>800 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text\n",
       "0                            seta to khubi bhaal have!\n",
       "1    m mase kono ullekhayogya tapapravaher dasha an...\n",
       "2                     ei sabkatai darun lagno shunate.\n",
       "3    tar prabandh, ya ki na tar agami bayer ekati u...\n",
       "4    bartaman mammla, njity taar ekhtiar parityag k...\n",
       "..                                                 ...\n",
       "795  1997lo sanyo \"pioessiepi\" palimar tantelu chip...\n",
       "796  nenu i pradeshal gurinchi ippatike chala chadi...\n",
       "797    gurtunda, edadi kindat me kolig biknu kalcharu.\n",
       "798  bharatvaesha gopp vaividhya-vyatyasabharit bhumi.\n",
       "799                             repe ambedkar jayanti!\n",
       "\n",
       "[800 rows x 1 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGS = ['ben', 'hin', 'pan', 'tam', 'tel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>seta to khubi bhaal have!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>m mase kono ullekhayogya tapapravaher dasha an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ei sabkatai darun lagno shunate.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tar prabandh, ya ki na tar agami bayer ekati u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bartaman mammla, njity taar ekhtiar parityag k...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0                          seta to khubi bhaal have!\n",
       "1  m mase kono ullekhayogya tapapravaher dasha an...\n",
       "2                   ei sabkatai darun lagno shunate.\n",
       "3  tar prabandh, ya ki na tar agami bayer ekati u...\n",
       "4  bartaman mammla, njity taar ekhtiar parityag k..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "dfs=[]\n",
    "for i,col_name in enumerate(df.columns):\n",
    "    if col_name in LANGS:\n",
    "        df2=pd.DataFrame({'Comment':df[col_name],'Language_Index': LANGS.index(col_name), 'Language': col_name})\n",
    "        dfs.append(df2)\n",
    "\n",
    "result_df = pd.concat(dfs, ignore_index=True)   \n",
    "print(len(dfs)) \n",
    "\n",
    "X=result_df['Comment']\n",
    "Y=result_df['Language_Index']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment</th>\n",
       "      <th>Language_Index</th>\n",
       "      <th>Language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sidny opera houser ashpashe thaka suijarlander...</td>\n",
       "      <td>0</td>\n",
       "      <td>ben</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>con jivanu upar kaaj karbe sei anuyayi aigulo ...</td>\n",
       "      <td>0</td>\n",
       "      <td>ben</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bharter drut arth lenden vyavastha upiaike any...</td>\n",
       "      <td>0</td>\n",
       "      <td>ben</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fasalpoorvavarti evan fasalparavarty caryakram...</td>\n",
       "      <td>0</td>\n",
       "      <td>ben</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>maldwar parikshar paddhati ekati drut paddhati...</td>\n",
       "      <td>0</td>\n",
       "      <td>ben</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Comment  Language_Index Language\n",
       "0  sidny opera houser ashpashe thaka suijarlander...               0      ben\n",
       "1  con jivanu upar kaaj karbe sei anuyayi aigulo ...               0      ben\n",
       "2  bharter drut arth lenden vyavastha upiaike any...               0      ben\n",
       "3  fasalpoorvavarti evan fasalparavarty caryakram...               0      ben\n",
       "4  maldwar parikshar paddhati ekati drut paddhati...               0      ben"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "def remove_tags(raw_text):\n",
    "    if not isinstance(raw_text, str):\n",
    "        raw_text = str(raw_text)\n",
    "    cleaned_text = re.sub(r'[^\\w\\s]', '', raw_text)\n",
    "    return cleaned_text\n",
    "\n",
    "result_df['Comment'] = result_df['Comment'].apply(remove_tags)\n",
    "result_df['Comment'] = result_df['Comment'].str.lower()\n",
    "result_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "USE result_df for further uses "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X_train_vect = vectorizer.fit_transform(X_train)\n",
    "X_test_vect = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.981764705882353\n"
     ]
    }
   ],
   "source": [
    "lr_classifier = LogisticRegression(max_iter=300)\n",
    "lr_classifier.fit(X_train_vect, y_train)\n",
    "pred_lr = lr_classifier.predict(X_test_vect)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, lr_classifier.predict(X_test_vect)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9905882352941177\n"
     ]
    }
   ],
   "source": [
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train_vect, y_train)\n",
    "pred_nb = nb_classifier.predict(X_test_vect)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, pred_nb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, GRU, Dense\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "train_df, test_df = train_test_split(result_df, test_size=0.2, random_state=24)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=15000, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(train_df['Comment'])\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(train_df['Comment'])\n",
    "test_sequences = tokenizer.texts_to_sequences(test_df['Comment'])\n",
    "\n",
    "max_length = 150  # Adjust this value based on the length to which you have padded your\n",
    "\n",
    "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "test_padded = pad_sequences(test_sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "train_labels = train_df['Language_Index']\n",
    "test_labels = test_df['Language_Index']\n",
    "\n",
    "# train_padded_3d = tf.expand_dims(train_padded, axis=-1)\n",
    "# test_padded_3d = tf.expand_dims(test_padded, axis=-1)\n",
    "\n",
    "train_padded_3d = train_padded\n",
    "test_padded_3d = test_padded\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embedding_dim = 170\n",
    "\n",
    "train_labels_one_hot = to_categorical(train_labels, num_classes=5)\n",
    "test_labels_one_hot = to_categorical(test_labels, num_classes=5)\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=embedding_dim),\n",
    "    Bidirectional(GRU(units=128, return_sequences=True)),\n",
    "    Bidirectional(GRU(units=256, return_sequences=True)),\n",
    "    Bidirectional(GRU(units=128)),\n",
    "    Dense(5, activation='softmax'),\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# input_length=max_length\n",
    "#The difference between sparse_categorical_crossentropy and categorical_crossentropy is whether your targets are one-hot encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels_one_hot[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_10                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_8 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_8 (\u001b[38;5;33mBidirectional\u001b[0m) │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_9 (\u001b[38;5;33mBidirectional\u001b[0m) │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_10                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1945s\u001b[0m 9s/step - accuracy: 0.6990 - loss: 0.7273 - val_accuracy: 0.9506 - val_loss: 0.1325\n",
      "Epoch 2/5\n",
      "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1528s\u001b[0m 7s/step - accuracy: 0.9817 - loss: 0.0574 - val_accuracy: 0.9453 - val_loss: 0.1799\n",
      "Epoch 3/5\n",
      "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1536s\u001b[0m 7s/step - accuracy: 0.9884 - loss: 0.0418 - val_accuracy: 0.9682 - val_loss: 0.1420\n",
      "Epoch 4/5\n",
      "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2037s\u001b[0m 10s/step - accuracy: 0.9948 - loss: 0.0165 - val_accuracy: 0.9724 - val_loss: 0.0979\n",
      "Epoch 5/5\n",
      "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1671s\u001b[0m 8s/step - accuracy: 0.9977 - loss: 0.0062 - val_accuracy: 0.9653 - val_loss: 0.1564\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Enable eager execution\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_padded_3d, train_labels_one_hot, epochs=5, batch_size=32, validation_data=(test_padded_3d, test_labels_one_hot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('tf.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\agrvi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\structured_function.py:258: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 3s/step\n",
      "GRU acc score:  0.9652941176470589\n",
      "GRU F1 score:  0.9655789554374445\n"
     ]
    }
   ],
   "source": [
    "pred_GRU = model.predict(test_padded)\n",
    "pred_GRU_2 = tf.argmax(pred_GRU, axis = 1)\n",
    "##reversing the OH encoding\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"GRU acc score: \", accuracy_score(test_labels, pred_GRU_2))\n",
    "print(\"GRU F1 score: \", f1_score(test_labels, pred_GRU_2,average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb5d4fbb9e044c27ada5fa1b9f69c934",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score:  0.9894222540839781\n",
      "Accuracy:  0.9894117647058823\n",
      "MSE:  0.05411764705882353\n"
     ]
    }
   ],
   "source": [
    "# NORMALIZE and MAKE between 0 and 1\n",
    "def prob(arr:np.ndarray, gap_adjuster:int=3)->np.ndarray:\n",
    "    if len(arr.shape) == 1:\n",
    "        arr = (arr-arr.min())/(arr.max()-arr.min())\n",
    "        if gap_adjuster!=1: arr = arr**gap_adjuster\n",
    "        return arr/arr.sum()\n",
    "    else:\n",
    "        arr = (arr-arr.min(axis=1).reshape(-1, 1))/(arr.max(axis=1)-arr.min(axis=1)).reshape(-1, 1)\n",
    "        if gap_adjuster!=1: arr = arr**gap_adjuster\n",
    "        return arr/arr.sum(axis=1).reshape(-1, 1)\n",
    "    \n",
    "\n",
    "def emsemble_infer_v2(texts:str|list[str], printable=False):\n",
    "    if isinstance(texts, str): texts = [texts]\n",
    "    output = (\n",
    "        prob(lr_classifier.predict_proba(vectorizer.transform(texts)), gap_adjuster=1) + \n",
    "        prob(nb_classifier.predict_proba(vectorizer.transform(texts)), gap_adjuster=1) \n",
    "        # prob(model.predict(texts), gap_adjuster=1)\n",
    "    ).argmax(axis=1)\n",
    "    if printable:\n",
    "        return [LANGS[i] for i in output.tolist()]\n",
    "    else:\n",
    "        return output\n",
    "    \n",
    "pred_emsemble_v2 = []\n",
    "for i in tqdm(range(0, len(X_test), 64)):\n",
    "    pred_emsemble_v2.append(emsemble_infer_v2(X_test[i:i+64]))\n",
    "\n",
    "pred_emsemble_v2 = np.concatenate(pred_emsemble_v2, axis = 0)\n",
    "\n",
    "print(\"F1 score: \", f1_score(y_test, pred_emsemble_v2, average='weighted'))\n",
    "print(\"Accuracy: \", accuracy_score(y_test, pred_emsemble_v2))\n",
    "print(\"MSE: \", mean_squared_error(y_test, pred_emsemble_v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_translitration=df_test['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83895b0953c94a559fd9430cf68e33bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_emsemble_v2 = []\n",
    "for i in tqdm(range(0, len(df_translitration), 64)):\n",
    "    pred_emsemble_v2.append(emsemble_infer_v2(df_translitration[i:i+64]))\n",
    "\n",
    "pred_emsemble_v2 = np.concatenate(pred_emsemble_v2, axis = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'ben', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'hin', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'hin', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'pan', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tam', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel', 'tel']\n"
     ]
    }
   ],
   "source": [
    "# Mapping of integers to labels\n",
    "label_mapping = {\n",
    "    idx:name for idx, name in enumerate(LANGS)\n",
    "}\n",
    "\n",
    "# Assuming `output` is the concatenated list of integers\n",
    "labels = [label_mapping[i] for i in pred_emsemble_v2 ]\n",
    "\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test1['Language'] = labels\n",
    "df_test1['Language_index']=pred_emsemble_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test1.to_csv(\"D:\\koding\\codes\\Machine Learning\\ICTC\\data\\classification_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Distillbert Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.25, random_state=42)  # 0.25 * 0.8 = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\agrvi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\accelerate\\accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=5)  # num_labels for multiclass classification\n",
    "\n",
    "# Tokenize the datasets\n",
    "train_encodings = tokenizer(train_df['Comment'].tolist(), truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_df['Comment'].tolist(), truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_df['Comment'].tolist(), truncation=True, padding=True)\n",
    "\n",
    "# Convert labels to tensor with correct dtype\n",
    "train_labels = torch.tensor(train_df['Language_Index'].tolist(), dtype=torch.long)\n",
    "val_labels = torch.tensor(val_df['Language_Index'].tolist(), dtype=torch.long)\n",
    "\n",
    "# Create torch dataset\n",
    "class LanguageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels is not None:\n",
    "            item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "train_dataset = LanguageDataset(train_encodings, train_labels)\n",
    "val_dataset = LanguageDataset(val_encodings, val_labels)\n",
    "test_dataset = LanguageDataset(test_encodings)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "# Create Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb580b7ce3ea4c9faaedde150e5cdd1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1914 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5921, 'grad_norm': 2.2322659492492676, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.02}\n",
      "{'loss': 1.5939, 'grad_norm': 2.4486351013183594, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.03}\n",
      "{'loss': 1.6226, 'grad_norm': 2.5752220153808594, 'learning_rate': 3e-06, 'epoch': 0.05}\n",
      "{'loss': 1.6019, 'grad_norm': 3.3775687217712402, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.06}\n",
      "{'loss': 1.5701, 'grad_norm': 3.0415420532226562, 'learning_rate': 5e-06, 'epoch': 0.08}\n",
      "{'loss': 1.5691, 'grad_norm': 2.3397762775421143, 'learning_rate': 6e-06, 'epoch': 0.09}\n",
      "{'loss': 1.5327, 'grad_norm': 4.01192045211792, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.11}\n",
      "{'loss': 1.461, 'grad_norm': 4.666281700134277, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.13}\n",
      "{'loss': 1.3894, 'grad_norm': 4.114907741546631, 'learning_rate': 9e-06, 'epoch': 0.14}\n",
      "{'loss': 1.2717, 'grad_norm': 7.083866596221924, 'learning_rate': 1e-05, 'epoch': 0.16}\n",
      "{'loss': 1.2421, 'grad_norm': 7.3499860763549805, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.17}\n",
      "{'loss': 1.0859, 'grad_norm': 4.241292953491211, 'learning_rate': 1.2e-05, 'epoch': 0.19}\n",
      "{'loss': 1.032, 'grad_norm': 11.81212329864502, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.2}\n",
      "{'loss': 0.9536, 'grad_norm': 6.350066184997559, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.22}\n",
      "{'loss': 0.8435, 'grad_norm': 3.3219380378723145, 'learning_rate': 1.5e-05, 'epoch': 0.24}\n",
      "{'loss': 0.807, 'grad_norm': 7.579165935516357, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.25}\n",
      "{'loss': 0.7037, 'grad_norm': 8.962750434875488, 'learning_rate': 1.7000000000000003e-05, 'epoch': 0.27}\n",
      "{'loss': 0.6969, 'grad_norm': 9.018239974975586, 'learning_rate': 1.8e-05, 'epoch': 0.28}\n",
      "{'loss': 0.6899, 'grad_norm': 4.199796676635742, 'learning_rate': 1.9e-05, 'epoch': 0.3}\n",
      "{'loss': 0.6446, 'grad_norm': 7.84765625, 'learning_rate': 2e-05, 'epoch': 0.31}\n",
      "{'loss': 0.5499, 'grad_norm': 8.521268844604492, 'learning_rate': 2.1e-05, 'epoch': 0.33}\n",
      "{'loss': 0.637, 'grad_norm': 16.197628021240234, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.34}\n",
      "{'loss': 0.5169, 'grad_norm': 12.0482177734375, 'learning_rate': 2.3000000000000003e-05, 'epoch': 0.36}\n",
      "{'loss': 0.6121, 'grad_norm': 16.397388458251953, 'learning_rate': 2.4e-05, 'epoch': 0.38}\n",
      "{'loss': 0.4562, 'grad_norm': 8.850895881652832, 'learning_rate': 2.5e-05, 'epoch': 0.39}\n",
      "{'loss': 0.4531, 'grad_norm': 2.713531970977783, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.41}\n",
      "{'loss': 0.3164, 'grad_norm': 2.3011765480041504, 'learning_rate': 2.7000000000000002e-05, 'epoch': 0.42}\n",
      "{'loss': 0.4129, 'grad_norm': 6.10883092880249, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.44}\n",
      "{'loss': 0.3828, 'grad_norm': 13.520336151123047, 'learning_rate': 2.9e-05, 'epoch': 0.45}\n",
      "{'loss': 0.3526, 'grad_norm': 7.08320951461792, 'learning_rate': 3e-05, 'epoch': 0.47}\n",
      "{'loss': 0.4066, 'grad_norm': 12.578001022338867, 'learning_rate': 3.1e-05, 'epoch': 0.49}\n",
      "{'loss': 0.2911, 'grad_norm': 7.80377721786499, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.5}\n",
      "{'loss': 0.3542, 'grad_norm': 4.051450729370117, 'learning_rate': 3.3e-05, 'epoch': 0.52}\n",
      "{'loss': 0.1867, 'grad_norm': 0.7073035836219788, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.53}\n",
      "{'loss': 0.2711, 'grad_norm': 13.93127155303955, 'learning_rate': 3.5e-05, 'epoch': 0.55}\n",
      "{'loss': 0.3619, 'grad_norm': 0.6287361979484558, 'learning_rate': 3.6e-05, 'epoch': 0.56}\n",
      "{'loss': 0.3198, 'grad_norm': 22.408674240112305, 'learning_rate': 3.7e-05, 'epoch': 0.58}\n",
      "{'loss': 0.337, 'grad_norm': 26.748008728027344, 'learning_rate': 3.8e-05, 'epoch': 0.6}\n",
      "{'loss': 0.2978, 'grad_norm': 20.097972869873047, 'learning_rate': 3.9000000000000006e-05, 'epoch': 0.61}\n",
      "{'loss': 0.2534, 'grad_norm': 0.2751046121120453, 'learning_rate': 4e-05, 'epoch': 0.63}\n",
      "{'loss': 0.1218, 'grad_norm': 1.8250023126602173, 'learning_rate': 4.1e-05, 'epoch': 0.64}\n",
      "{'loss': 0.1319, 'grad_norm': 0.48287534713745117, 'learning_rate': 4.2e-05, 'epoch': 0.66}\n",
      "{'loss': 0.3506, 'grad_norm': 0.13946639001369476, 'learning_rate': 4.3e-05, 'epoch': 0.67}\n",
      "{'loss': 0.2696, 'grad_norm': 21.799156188964844, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.69}\n",
      "{'loss': 0.5021, 'grad_norm': 0.9362019300460815, 'learning_rate': 4.5e-05, 'epoch': 0.71}\n",
      "{'loss': 0.3488, 'grad_norm': 6.443691730499268, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.72}\n",
      "{'loss': 0.324, 'grad_norm': 56.02642059326172, 'learning_rate': 4.7e-05, 'epoch': 0.74}\n",
      "{'loss': 0.4255, 'grad_norm': 62.346702575683594, 'learning_rate': 4.8e-05, 'epoch': 0.75}\n",
      "{'loss': 0.2221, 'grad_norm': 0.4649110436439514, 'learning_rate': 4.9e-05, 'epoch': 0.77}\n",
      "{'loss': 0.2853, 'grad_norm': 1.2858856916427612, 'learning_rate': 5e-05, 'epoch': 0.78}\n",
      "{'loss': 0.2079, 'grad_norm': 2.206270456314087, 'learning_rate': 4.9646393210749645e-05, 'epoch': 0.8}\n",
      "{'loss': 0.1985, 'grad_norm': 0.9693201184272766, 'learning_rate': 4.9292786421499294e-05, 'epoch': 0.82}\n",
      "{'loss': 0.1696, 'grad_norm': 0.10960999876260757, 'learning_rate': 4.893917963224894e-05, 'epoch': 0.83}\n",
      "{'loss': 0.2886, 'grad_norm': 13.190184593200684, 'learning_rate': 4.858557284299859e-05, 'epoch': 0.85}\n",
      "{'loss': 0.3449, 'grad_norm': 36.7003059387207, 'learning_rate': 4.8231966053748234e-05, 'epoch': 0.86}\n",
      "{'loss': 0.2244, 'grad_norm': 13.093131065368652, 'learning_rate': 4.787835926449788e-05, 'epoch': 0.88}\n",
      "{'loss': 0.2629, 'grad_norm': 0.07867680490016937, 'learning_rate': 4.7524752475247525e-05, 'epoch': 0.89}\n",
      "{'loss': 0.3369, 'grad_norm': 3.9449222087860107, 'learning_rate': 4.7171145685997174e-05, 'epoch': 0.91}\n",
      "{'loss': 0.2647, 'grad_norm': 1.8472645282745361, 'learning_rate': 4.681753889674682e-05, 'epoch': 0.92}\n",
      "{'loss': 0.2216, 'grad_norm': 14.28691577911377, 'learning_rate': 4.6463932107496465e-05, 'epoch': 0.94}\n",
      "{'loss': 0.2432, 'grad_norm': 0.26741498708724976, 'learning_rate': 4.6110325318246114e-05, 'epoch': 0.96}\n",
      "{'loss': 0.2616, 'grad_norm': 4.964288711547852, 'learning_rate': 4.5756718528995756e-05, 'epoch': 0.97}\n",
      "{'loss': 0.081, 'grad_norm': 0.06356486678123474, 'learning_rate': 4.5403111739745405e-05, 'epoch': 0.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb1c15162a044851b8acfcc85175fb90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/213 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.21645082533359528, 'eval_runtime': 252.6863, 'eval_samples_per_second': 6.728, 'eval_steps_per_second': 0.843, 'epoch': 1.0}\n",
      "{'loss': 0.2404, 'grad_norm': 0.067100390791893, 'learning_rate': 4.5049504950495054e-05, 'epoch': 1.0}\n",
      "{'loss': 0.3096, 'grad_norm': 0.1992466151714325, 'learning_rate': 4.4695898161244696e-05, 'epoch': 1.02}\n",
      "{'loss': 0.2784, 'grad_norm': 1.3101789951324463, 'learning_rate': 4.4342291371994345e-05, 'epoch': 1.03}\n",
      "{'loss': 0.1446, 'grad_norm': 0.47412362694740295, 'learning_rate': 4.398868458274399e-05, 'epoch': 1.05}\n",
      "{'loss': 0.1524, 'grad_norm': 0.09020315110683441, 'learning_rate': 4.363507779349364e-05, 'epoch': 1.07}\n",
      "{'loss': 0.3214, 'grad_norm': 0.28224310278892517, 'learning_rate': 4.3281471004243286e-05, 'epoch': 1.08}\n",
      "{'loss': 0.5032, 'grad_norm': 0.0929035022854805, 'learning_rate': 4.292786421499293e-05, 'epoch': 1.1}\n",
      "{'loss': 0.1309, 'grad_norm': 21.733591079711914, 'learning_rate': 4.257425742574258e-05, 'epoch': 1.11}\n",
      "{'loss': 0.2018, 'grad_norm': 37.1757698059082, 'learning_rate': 4.222065063649222e-05, 'epoch': 1.13}\n",
      "{'loss': 0.1684, 'grad_norm': 0.054063811898231506, 'learning_rate': 4.186704384724187e-05, 'epoch': 1.14}\n",
      "{'loss': 0.015, 'grad_norm': 0.05877140536904335, 'learning_rate': 4.151343705799152e-05, 'epoch': 1.16}\n",
      "{'loss': 0.2078, 'grad_norm': 4.426421642303467, 'learning_rate': 4.115983026874116e-05, 'epoch': 1.18}\n",
      "{'loss': 0.2154, 'grad_norm': 1.2851049900054932, 'learning_rate': 4.080622347949081e-05, 'epoch': 1.19}\n",
      "{'loss': 0.1211, 'grad_norm': 0.28792163729667664, 'learning_rate': 4.045261669024045e-05, 'epoch': 1.21}\n",
      "{'loss': 0.2096, 'grad_norm': 0.3209778964519501, 'learning_rate': 4.0099009900990106e-05, 'epoch': 1.22}\n",
      "{'loss': 0.037, 'grad_norm': 0.8229969143867493, 'learning_rate': 3.974540311173975e-05, 'epoch': 1.24}\n",
      "{'loss': 0.0857, 'grad_norm': 0.023777293041348457, 'learning_rate': 3.939179632248939e-05, 'epoch': 1.25}\n",
      "{'loss': 0.136, 'grad_norm': 0.018210144713521004, 'learning_rate': 3.903818953323904e-05, 'epoch': 1.27}\n",
      "{'loss': 0.3087, 'grad_norm': 24.457277297973633, 'learning_rate': 3.868458274398868e-05, 'epoch': 1.29}\n",
      "{'loss': 0.1787, 'grad_norm': 30.89705467224121, 'learning_rate': 3.833097595473834e-05, 'epoch': 1.3}\n",
      "{'loss': 0.0902, 'grad_norm': 0.021856755018234253, 'learning_rate': 3.797736916548798e-05, 'epoch': 1.32}\n",
      "{'loss': 0.0131, 'grad_norm': 0.12780821323394775, 'learning_rate': 3.762376237623763e-05, 'epoch': 1.33}\n",
      "{'loss': 0.0387, 'grad_norm': 0.05902381241321564, 'learning_rate': 3.727015558698727e-05, 'epoch': 1.35}\n",
      "{'loss': 0.0983, 'grad_norm': 21.801362991333008, 'learning_rate': 3.691654879773691e-05, 'epoch': 1.36}\n",
      "{'loss': 0.084, 'grad_norm': 0.04036765918135643, 'learning_rate': 3.656294200848657e-05, 'epoch': 1.38}\n",
      "{'loss': 0.0967, 'grad_norm': 0.3578833043575287, 'learning_rate': 3.620933521923621e-05, 'epoch': 1.39}\n",
      "{'loss': 0.2807, 'grad_norm': 0.046264927834272385, 'learning_rate': 3.585572842998586e-05, 'epoch': 1.41}\n",
      "{'loss': 0.1746, 'grad_norm': 20.001283645629883, 'learning_rate': 3.55021216407355e-05, 'epoch': 1.43}\n",
      "{'loss': 0.0537, 'grad_norm': 0.029330842196941376, 'learning_rate': 3.514851485148515e-05, 'epoch': 1.44}\n",
      "{'loss': 0.1437, 'grad_norm': 0.038594476878643036, 'learning_rate': 3.47949080622348e-05, 'epoch': 1.46}\n",
      "{'loss': 0.1795, 'grad_norm': 0.5256089568138123, 'learning_rate': 3.444130127298444e-05, 'epoch': 1.47}\n",
      "{'loss': 0.2256, 'grad_norm': 0.06649873405694962, 'learning_rate': 3.408769448373409e-05, 'epoch': 1.49}\n",
      "{'loss': 0.3328, 'grad_norm': 0.323701411485672, 'learning_rate': 3.3734087694483734e-05, 'epoch': 1.5}\n",
      "{'loss': 0.1189, 'grad_norm': 0.04992778226733208, 'learning_rate': 3.338048090523338e-05, 'epoch': 1.52}\n",
      "{'loss': 0.1656, 'grad_norm': 0.05498902499675751, 'learning_rate': 3.302687411598303e-05, 'epoch': 1.54}\n",
      "{'loss': 0.1224, 'grad_norm': 0.049768153578042984, 'learning_rate': 3.2673267326732674e-05, 'epoch': 1.55}\n",
      "{'loss': 0.17, 'grad_norm': 0.05068498104810715, 'learning_rate': 3.231966053748232e-05, 'epoch': 1.57}\n",
      "{'loss': 0.0178, 'grad_norm': 0.062471095472574234, 'learning_rate': 3.1966053748231965e-05, 'epoch': 1.58}\n",
      "{'loss': 0.0218, 'grad_norm': 0.026090772822499275, 'learning_rate': 3.1612446958981614e-05, 'epoch': 1.6}\n",
      "{'loss': 0.2497, 'grad_norm': 0.056816376745700836, 'learning_rate': 3.125884016973126e-05, 'epoch': 1.61}\n",
      "{'loss': 0.0716, 'grad_norm': 0.022819895297288895, 'learning_rate': 3.0905233380480905e-05, 'epoch': 1.63}\n",
      "{'loss': 0.2876, 'grad_norm': 39.78350067138672, 'learning_rate': 3.0551626591230554e-05, 'epoch': 1.65}\n",
      "{'loss': 0.0333, 'grad_norm': 0.024505922570824623, 'learning_rate': 3.01980198019802e-05, 'epoch': 1.66}\n",
      "{'loss': 0.0247, 'grad_norm': 17.37734603881836, 'learning_rate': 2.9844413012729845e-05, 'epoch': 1.68}\n",
      "{'loss': 0.2564, 'grad_norm': 0.06638329476118088, 'learning_rate': 2.9490806223479494e-05, 'epoch': 1.69}\n",
      "{'loss': 0.0945, 'grad_norm': 0.023366739973425865, 'learning_rate': 2.913719943422914e-05, 'epoch': 1.71}\n",
      "{'loss': 0.0027, 'grad_norm': 0.08607691526412964, 'learning_rate': 2.8783592644978786e-05, 'epoch': 1.72}\n",
      "{'loss': 0.1949, 'grad_norm': 5.711617469787598, 'learning_rate': 2.842998585572843e-05, 'epoch': 1.74}\n",
      "{'loss': 0.0576, 'grad_norm': 0.015522075816988945, 'learning_rate': 2.8076379066478077e-05, 'epoch': 1.76}\n",
      "{'loss': 0.0206, 'grad_norm': 0.061407603323459625, 'learning_rate': 2.7722772277227726e-05, 'epoch': 1.77}\n",
      "{'loss': 0.0598, 'grad_norm': 0.08039135485887527, 'learning_rate': 2.736916548797737e-05, 'epoch': 1.79}\n",
      "{'loss': 0.0983, 'grad_norm': 0.017606504261493683, 'learning_rate': 2.7015558698727017e-05, 'epoch': 1.8}\n",
      "{'loss': 0.1519, 'grad_norm': 11.384970664978027, 'learning_rate': 2.6661951909476662e-05, 'epoch': 1.82}\n",
      "{'loss': 0.0605, 'grad_norm': 0.018938247114419937, 'learning_rate': 2.6308345120226308e-05, 'epoch': 1.83}\n",
      "{'loss': 0.1924, 'grad_norm': 0.01253761351108551, 'learning_rate': 2.5954738330975957e-05, 'epoch': 1.85}\n",
      "{'loss': 0.1052, 'grad_norm': 0.027695538476109505, 'learning_rate': 2.5601131541725603e-05, 'epoch': 1.87}\n",
      "{'loss': 0.0134, 'grad_norm': 0.05923318862915039, 'learning_rate': 2.5247524752475248e-05, 'epoch': 1.88}\n",
      "{'loss': 0.0613, 'grad_norm': 0.01632910780608654, 'learning_rate': 2.4893917963224894e-05, 'epoch': 1.9}\n",
      "{'loss': 0.0471, 'grad_norm': 0.027205243706703186, 'learning_rate': 2.4540311173974543e-05, 'epoch': 1.91}\n",
      "{'loss': 0.0344, 'grad_norm': 0.09273368865251541, 'learning_rate': 2.418670438472419e-05, 'epoch': 1.93}\n",
      "{'loss': 0.001, 'grad_norm': 0.008773516863584518, 'learning_rate': 2.3833097595473834e-05, 'epoch': 1.94}\n",
      "{'loss': 0.1694, 'grad_norm': 16.032060623168945, 'learning_rate': 2.347949080622348e-05, 'epoch': 1.96}\n",
      "{'loss': 0.0009, 'grad_norm': 0.009740986861288548, 'learning_rate': 2.3125884016973125e-05, 'epoch': 1.97}\n",
      "{'loss': 0.0823, 'grad_norm': 0.03266370669007301, 'learning_rate': 2.2772277227722774e-05, 'epoch': 1.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "072c700f722b4ab9a8d0385078f0278c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/213 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.09549234062433243, 'eval_runtime': 360.8659, 'eval_samples_per_second': 4.711, 'eval_steps_per_second': 0.59, 'epoch': 2.0}\n",
      "{'loss': 0.0089, 'grad_norm': 0.022835101932287216, 'learning_rate': 2.241867043847242e-05, 'epoch': 2.01}\n",
      "{'loss': 0.1198, 'grad_norm': 5.262302875518799, 'learning_rate': 2.206506364922207e-05, 'epoch': 2.02}\n",
      "{'loss': 0.0744, 'grad_norm': 0.0481322780251503, 'learning_rate': 2.1711456859971714e-05, 'epoch': 2.04}\n",
      "{'loss': 0.0446, 'grad_norm': 0.012124293483793736, 'learning_rate': 2.1357850070721357e-05, 'epoch': 2.05}\n",
      "{'loss': 0.0083, 'grad_norm': 0.05686793476343155, 'learning_rate': 2.1004243281471006e-05, 'epoch': 2.07}\n",
      "{'loss': 0.0058, 'grad_norm': 0.009685027413070202, 'learning_rate': 2.065063649222065e-05, 'epoch': 2.08}\n",
      "{'loss': 0.0033, 'grad_norm': 0.009279572404921055, 'learning_rate': 2.02970297029703e-05, 'epoch': 2.1}\n",
      "{'loss': 0.1444, 'grad_norm': 18.82878303527832, 'learning_rate': 1.9943422913719946e-05, 'epoch': 2.12}\n",
      "{'loss': 0.0225, 'grad_norm': 8.507719039916992, 'learning_rate': 1.958981612446959e-05, 'epoch': 2.13}\n",
      "{'loss': 0.0198, 'grad_norm': 70.6264877319336, 'learning_rate': 1.9236209335219237e-05, 'epoch': 2.15}\n",
      "{'loss': 0.0788, 'grad_norm': 0.016062883660197258, 'learning_rate': 1.8882602545968883e-05, 'epoch': 2.16}\n",
      "{'loss': 0.0084, 'grad_norm': 0.008905150927603245, 'learning_rate': 1.8528995756718528e-05, 'epoch': 2.18}\n",
      "{'loss': 0.0386, 'grad_norm': 0.011968889273703098, 'learning_rate': 1.8175388967468177e-05, 'epoch': 2.19}\n",
      "{'loss': 0.0531, 'grad_norm': 0.00796725694090128, 'learning_rate': 1.7821782178217823e-05, 'epoch': 2.21}\n",
      "{'loss': 0.0144, 'grad_norm': 0.027592068538069725, 'learning_rate': 1.7468175388967468e-05, 'epoch': 2.23}\n",
      "{'loss': 0.0007, 'grad_norm': 0.0067819273099303246, 'learning_rate': 1.7114568599717114e-05, 'epoch': 2.24}\n",
      "{'loss': 0.0211, 'grad_norm': 0.014099987223744392, 'learning_rate': 1.676096181046676e-05, 'epoch': 2.26}\n",
      "{'loss': 0.0011, 'grad_norm': 0.009827129542827606, 'learning_rate': 1.640735502121641e-05, 'epoch': 2.27}\n",
      "{'loss': 0.0259, 'grad_norm': 0.012243075296282768, 'learning_rate': 1.6053748231966054e-05, 'epoch': 2.29}\n",
      "{'loss': 0.1116, 'grad_norm': 0.01696609891951084, 'learning_rate': 1.5700141442715703e-05, 'epoch': 2.3}\n",
      "{'loss': 0.0611, 'grad_norm': 0.014527436345815659, 'learning_rate': 1.534653465346535e-05, 'epoch': 2.32}\n",
      "{'loss': 0.0244, 'grad_norm': 0.009378230199217796, 'learning_rate': 1.4992927864214993e-05, 'epoch': 2.34}\n",
      "{'loss': 0.0026, 'grad_norm': 0.008675909601151943, 'learning_rate': 1.463932107496464e-05, 'epoch': 2.35}\n",
      "{'loss': 0.0007, 'grad_norm': 0.0092864278703928, 'learning_rate': 1.4285714285714285e-05, 'epoch': 2.37}\n",
      "{'loss': 0.0866, 'grad_norm': 8.779725074768066, 'learning_rate': 1.3932107496463934e-05, 'epoch': 2.38}\n",
      "{'loss': 0.0008, 'grad_norm': 0.009206067770719528, 'learning_rate': 1.3578500707213578e-05, 'epoch': 2.4}\n",
      "{'loss': 0.0644, 'grad_norm': 10.625316619873047, 'learning_rate': 1.3224893917963224e-05, 'epoch': 2.41}\n",
      "{'loss': 0.0919, 'grad_norm': 0.00790470466017723, 'learning_rate': 1.2871287128712873e-05, 'epoch': 2.43}\n",
      "{'loss': 0.0265, 'grad_norm': 0.007353190332651138, 'learning_rate': 1.2517680339462517e-05, 'epoch': 2.45}\n",
      "{'loss': 0.098, 'grad_norm': 0.007927131839096546, 'learning_rate': 1.2164073550212164e-05, 'epoch': 2.46}\n",
      "{'loss': 0.12, 'grad_norm': 0.008537445217370987, 'learning_rate': 1.1810466760961811e-05, 'epoch': 2.48}\n",
      "{'loss': 0.0005, 'grad_norm': 0.011501409113407135, 'learning_rate': 1.1456859971711457e-05, 'epoch': 2.49}\n",
      "{'loss': 0.0009, 'grad_norm': 0.009555244818329811, 'learning_rate': 1.1103253182461104e-05, 'epoch': 2.51}\n",
      "{'loss': 0.0346, 'grad_norm': 0.0056917704641819, 'learning_rate': 1.0749646393210752e-05, 'epoch': 2.52}\n",
      "{'loss': 0.0007, 'grad_norm': 0.007061830721795559, 'learning_rate': 1.0396039603960395e-05, 'epoch': 2.54}\n",
      "{'loss': 0.0653, 'grad_norm': 0.01383979432284832, 'learning_rate': 1.0042432814710043e-05, 'epoch': 2.55}\n",
      "{'loss': 0.0006, 'grad_norm': 0.05110578611493111, 'learning_rate': 9.68882602545969e-06, 'epoch': 2.57}\n",
      "{'loss': 0.0103, 'grad_norm': 0.007245942018926144, 'learning_rate': 9.335219236209336e-06, 'epoch': 2.59}\n",
      "{'loss': 0.0007, 'grad_norm': 0.06282275915145874, 'learning_rate': 8.981612446958983e-06, 'epoch': 2.6}\n",
      "{'loss': 0.0298, 'grad_norm': 0.02377214841544628, 'learning_rate': 8.628005657708629e-06, 'epoch': 2.62}\n",
      "{'loss': 0.0136, 'grad_norm': 1.5530139207839966, 'learning_rate': 8.274398868458274e-06, 'epoch': 2.63}\n",
      "{'loss': 0.053, 'grad_norm': 53.12852096557617, 'learning_rate': 7.920792079207921e-06, 'epoch': 2.65}\n",
      "{'loss': 0.0012, 'grad_norm': 0.0058844550512731075, 'learning_rate': 7.567185289957568e-06, 'epoch': 2.66}\n",
      "{'loss': 0.0819, 'grad_norm': 0.021448979154229164, 'learning_rate': 7.213578500707214e-06, 'epoch': 2.68}\n",
      "{'loss': 0.0038, 'grad_norm': 0.010604053735733032, 'learning_rate': 6.85997171145686e-06, 'epoch': 2.7}\n",
      "{'loss': 0.0007, 'grad_norm': 0.00795192550867796, 'learning_rate': 6.506364922206506e-06, 'epoch': 2.71}\n",
      "{'loss': 0.0005, 'grad_norm': 0.006739105563610792, 'learning_rate': 6.152758132956153e-06, 'epoch': 2.73}\n",
      "{'loss': 0.0383, 'grad_norm': 0.009871287271380424, 'learning_rate': 5.799151343705799e-06, 'epoch': 2.74}\n",
      "{'loss': 0.0009, 'grad_norm': 0.13940268754959106, 'learning_rate': 5.445544554455446e-06, 'epoch': 2.76}\n",
      "{'loss': 0.0197, 'grad_norm': 0.005074234679341316, 'learning_rate': 5.091937765205092e-06, 'epoch': 2.77}\n",
      "{'loss': 0.0056, 'grad_norm': 0.014038811437785625, 'learning_rate': 4.7383309759547385e-06, 'epoch': 2.79}\n",
      "{'loss': 0.0208, 'grad_norm': 0.006345403380692005, 'learning_rate': 4.384724186704385e-06, 'epoch': 2.81}\n",
      "{'loss': 0.001, 'grad_norm': 0.007437995634973049, 'learning_rate': 4.031117397454031e-06, 'epoch': 2.82}\n",
      "{'loss': 0.0013, 'grad_norm': 0.016318757086992264, 'learning_rate': 3.677510608203678e-06, 'epoch': 2.84}\n",
      "{'loss': 0.0006, 'grad_norm': 0.1056118831038475, 'learning_rate': 3.3239038189533243e-06, 'epoch': 2.85}\n",
      "{'loss': 0.0005, 'grad_norm': 0.011504012160003185, 'learning_rate': 2.9702970297029703e-06, 'epoch': 2.87}\n",
      "{'loss': 0.001, 'grad_norm': 0.005357285961508751, 'learning_rate': 2.6166902404526168e-06, 'epoch': 2.88}\n",
      "{'loss': 0.1356, 'grad_norm': 0.014794765040278435, 'learning_rate': 2.263083451202263e-06, 'epoch': 2.9}\n",
      "{'loss': 0.0191, 'grad_norm': 0.010719319805502892, 'learning_rate': 1.9094766619519096e-06, 'epoch': 2.92}\n",
      "{'loss': 0.1093, 'grad_norm': 0.005667195189744234, 'learning_rate': 1.5558698727015559e-06, 'epoch': 2.93}\n",
      "{'loss': 0.0161, 'grad_norm': 0.006308355368673801, 'learning_rate': 1.2022630834512023e-06, 'epoch': 2.95}\n",
      "{'loss': 0.0254, 'grad_norm': 0.006584295071661472, 'learning_rate': 8.486562942008486e-07, 'epoch': 2.96}\n",
      "{'loss': 0.0037, 'grad_norm': 0.00483765359967947, 'learning_rate': 4.950495049504951e-07, 'epoch': 2.98}\n",
      "{'loss': 0.0005, 'grad_norm': 0.006708501372486353, 'learning_rate': 1.4144271570014145e-07, 'epoch': 2.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bd88102f0ed4d24bf6daf1f2d6d387f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/213 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.09755615890026093, 'eval_runtime': 250.3552, 'eval_samples_per_second': 6.79, 'eval_steps_per_second': 0.851, 'epoch': 3.0}\n",
      "{'train_runtime': 10464.9105, 'train_samples_per_second': 1.462, 'train_steps_per_second': 0.183, 'train_loss': 0.25394987023244303, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1914, training_loss=0.25394987023244303, metrics={'train_runtime': 10464.9105, 'train_samples_per_second': 1.462, 'train_steps_per_second': 0.183, 'train_loss': 0.25394987023244303, 'epoch': 3.0})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b2bbaafd20046a6be5951b9795efb69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/213 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.09755615890026093,\n",
       " 'eval_runtime': 244.8817,\n",
       " 'eval_samples_per_second': 6.942,\n",
       " 'eval_steps_per_second': 0.87,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
